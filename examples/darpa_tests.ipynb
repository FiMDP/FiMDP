{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for DARPA Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fimdp.UUVEnv import Env\n",
    "from IPython import display\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from fimdp.energy_solver import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP Domain and Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MDP models the dynamics of an unmanned underwater vehicle (UUV) moving with a constant horizontal velocity in oceans whose current velocity vector is known apriori. The region of interest is discretized into a 2-dimensional grid where each cell in the grid forms a state in the MDP. Movement in north, south, east, and west directions form the action space. The actions are of two kinds: strong and weak. Strong actions, while consuming more energy, lead to deterministic outcomes where as weak actions have stochastic outcomes while consuming less energy. We first generate an example MDP using the following syntax and convert it into a consMDP object. In this MDP, a single target state near one corner of the grid and the reload state (which is also the initial state of the agent) is near the other corner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Variables\n",
    "\n",
    "# Essential\n",
    "grid_size = (20,20) # size of the grid\n",
    "agent_capacity = 150 # maximum energy capacity of the agent\n",
    "init_state = grid_size[0]*5+2 # Initial state of the agent\n",
    "reload_list = [grid_size[0]*5+2, grid_size[0]*grid_size[1] - 4*grid_size[0] - 10] # list of reload states in the MDP\n",
    "target_list = [grid_size[0]*grid_size[1] - 3*grid_size[0] - 6, grid_size[0]*grid_size[1] - 3*grid_size[0] - 8] # list of target states in the MDP\n",
    "\n",
    "# Non essential \n",
    "agent_velocity = 5 # horizontal velocity of the glider\n",
    "heading_sd = 0.524 # standard deviation in agent true heading\n",
    "weakaction_cost= 1 # cost of weak action of the agent\n",
    "strongaction_cost = 2 # cost of strong action of the agent\n",
    "\n",
    "# Generate MDP and export to consMDP\n",
    "env = Env(grid_size, agent_velocity, heading_sd, agent_capacity, weakaction_cost, strongaction_cost, reload_list, target_list, init_state=init_state, render=False)\n",
    "m, targets = env.create_consmdp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy Synthesis and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate the strategy using the 'EnergySolver' variant solver for almost-sure reachability objective for the given capacity and use the strategy to reach the target state. The following two functions help us in implementing the strategy and visualizing the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to take action based on the strategy\n",
    "def policy(strategy,state,energy):\n",
    "    \n",
    "    data_dict = strategy[state]\n",
    "    dict_keys = list(data_dict.keys())\n",
    "    \n",
    "    if not bool(dict_keys):\n",
    "        raise Exception('Strategy does not prescribe a safe action at this state. Increase the capacity of the agent.')\n",
    "    feasible = []\n",
    "    for value in dict_keys:\n",
    "        if value <= energy:\n",
    "            feasible.append(value)\n",
    "    \n",
    "    action_string = data_dict[max(feasible)]\n",
    "    agent_action = ast.literal_eval(action_string)[1]\n",
    "    \n",
    "    return agent_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to render the grid world\n",
    "def rendering(env, time_count):\n",
    "\n",
    "    data = -3*np.ones([env.grid_size[0],env.grid_size[1]])\n",
    "    for cell in env.states_history:\n",
    "        data[cell//env.grid_size[0], cell%env.grid_size[1]] = 3\n",
    "        data[env.position//env.grid_size[0], env.position%env.grid_size[1]] = -1\n",
    "    for cell in env.target_list:\n",
    "        data[cell//env.grid_size[0], cell%env.grid_size[1]] = 1\n",
    "    for cell in env.reload_list:\n",
    "        data[cell//env.grid_size[0], cell%env.grid_size[1]] = 0            \n",
    "\n",
    "    myColors = ['#A9A9A9',(0.7, 0.0, 0.0, 1.0),(0.7, 0.3, 0.0, 1.0), (0.0, 0.8, 0.0, 1.0),'#5C5959']\n",
    "    cmap = ListedColormap(myColors)\n",
    "    plt.clf()\n",
    "    ax = sns.heatmap(data, cmap=cmap, cbar =False, linewidths=.5, linecolor='lightgray')\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.set_title(\"Agent motion: t = {}\".format(time_count))\n",
    "    plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the red cell indicates the current location of the agent, green cells indicate target states, and orange cells indicate reload states. The history of the agent is indicated by dark grey cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target reached.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate strategy\n",
    "s = EnergySolver(m, cap=agent_capacity, targets=targets)\n",
    "strategy = s.get_strategy(AS_REACH, recompute=True)\n",
    "\n",
    "# Run simulation using the strategy\n",
    "time_count = 0\n",
    "env.reset(init_state)\n",
    "\n",
    "while True:\n",
    "       \n",
    "    energy = env.agent_energy\n",
    "    state = env.position\n",
    "     # info = (next_state, action taken, reward, done)\n",
    "    info = env.step(policy(strategy,state,energy)) # take action\n",
    "    time_count += 1\n",
    "\n",
    "    if info[0] in target_list:\n",
    "        print(\"Target reached.\")\n",
    "        rendering(env, time_count)\n",
    "        break\n",
    "    elif info[3] == 1:\n",
    "        print(\"Energy exhausted.\")\n",
    "        rendering(env, time_count)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
